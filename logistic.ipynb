{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from format_data import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation et formattage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/GAN_train.csv\")\n",
    "train_data = format_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe pour la régression logistique en utilisant seulement numpy et scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, n_iterations=100000, regularization=None, reg_coeff=0.01, \n",
    "                 weights=False, early_stopping=True, patience=500, verbose=False):\n",
    "        \"\"\"\n",
    "        Softmax Regression Classifier Initialization\n",
    "        \n",
    "        Parameters:\n",
    "        - n_iterations: maximum number of iterations for optimization\n",
    "        - regularization: regularization type ('L1' or 'L2')\n",
    "        - reg_coeff: regularization coefficient\n",
    "        - weights: boolean indicating if instance weights should be used\n",
    "        - early_stopping: boolean indicating if early stopping should be used\n",
    "        - patience: number of iterations without improvement to trigger early stopping\n",
    "        \"\"\"\n",
    "        self.learning_rate = None  # We'll be using the custom learning rate function\n",
    "        self.n_iterations = n_iterations\n",
    "        self.regularization = regularization\n",
    "        self.reg_coeff = reg_coeff\n",
    "        self.use_weights = weights\n",
    "        self.sample_weights = None\n",
    "        self.theta = None\n",
    "        self.early_stopping = early_stopping\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.best_iteration = None\n",
    "\n",
    "    def softmax(self, scores):\n",
    "        \"\"\"\n",
    "        Computes softmax probabilities for given scores\n",
    "        \n",
    "        Parameters:\n",
    "        - scores: raw score values\n",
    "        \n",
    "        Returns:\n",
    "        - Softmax probabilities\n",
    "        \"\"\"\n",
    "        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_class_weights(self, y):\n",
    "        \"\"\"\n",
    "        Computes weights for instances based on their class\n",
    "        \n",
    "        Parameters:\n",
    "        - y: class labels\n",
    "        \n",
    "        Returns:\n",
    "        - Array of instance weights\n",
    "        \"\"\"\n",
    "        class_sample_counts = np.bincount(y)\n",
    "        weights = 1. / class_sample_counts\n",
    "        weights = weights / np.sum(weights) * len(np.unique(y))\n",
    "        return np.array([weights[label] for label in y])\n",
    "\n",
    "    def get_custom_learning_rate(self, iteration):\n",
    "        \"\"\"Get custom learning rate based on current iteration\"\"\"\n",
    "        if iteration < 25:\n",
    "            return 0.1\n",
    "        elif iteration < 50:\n",
    "            return 0.05\n",
    "        elif iteration < 150:\n",
    "            return 0.01\n",
    "        elif iteration < 300:\n",
    "            return 0.005\n",
    "        else:\n",
    "            return 0.001\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Trains the softmax regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - X_train: training data features\n",
    "        - y_train: training data labels\n",
    "        - X_val: validation data features (optional)\n",
    "        - y_val: validation data labels (optional)\n",
    "        \"\"\"\n",
    "        # Convert to sparse matrix if not already\n",
    "        if not isinstance(X_train, csr_matrix):\n",
    "            X_train = csr_matrix(X_train)\n",
    "\n",
    "        # Add bias term (intercept) to training data\n",
    "        bias_train = csr_matrix(np.ones((X_train.shape[0], 1)))\n",
    "        X_train_bias = hstack([bias_train, X_train])\n",
    "\n",
    "        n_samples, n_features = X_train_bias.shape\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        \n",
    "        # Initialize weights with small random values\n",
    "        self.theta = np.random.randn(n_features, n_classes) * 0.01\n",
    "\n",
    "        # Compute or set sample weights\n",
    "        if self.use_weights:\n",
    "            self.sample_weights = self.compute_class_weights(y_train)\n",
    "        else:\n",
    "            self.sample_weights = np.ones(n_samples)\n",
    "\n",
    "        # Convert labels to one-hot encoding\n",
    "        y_onehot = np.zeros((n_samples, n_classes))\n",
    "        y_onehot[np.arange(n_samples), y_train] = 1\n",
    "\n",
    "        best_theta = None\n",
    "        best_val_accuracy = float('-inf')\n",
    "        no_improvement_count = 0\n",
    "\n",
    "        # Prepare validation data if early stopping is enabled\n",
    "        if self.early_stopping and X_val is not None and y_val is not None:\n",
    "            if not isinstance(X_val, csr_matrix):\n",
    "                X_val = csr_matrix(X_val)\n",
    "            bias_val = csr_matrix(np.ones((X_val.shape[0], 1)))\n",
    "            X_val_bias = hstack([bias_val, X_val])\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            # Update learning rate using custom logic\n",
    "            self.learning_rate = self.get_custom_learning_rate(i)\n",
    "\n",
    "            # Compute predictions using current weights\n",
    "            scores = X_train_bias.dot(self.theta)\n",
    "            probabilities = self.softmax(scores)\n",
    "\n",
    "            # Compute gradient for optimization\n",
    "            diff_weighted = self.sample_weights[:, np.newaxis] * (y_onehot - probabilities)\n",
    "            gradient = -X_train_bias.T.dot(diff_weighted) / n_samples\n",
    "\n",
    "            # Apply regularization to gradient if specified\n",
    "            if self.regularization == 'L2':\n",
    "                gradient[1:] += self.reg_coeff * self.theta[1:]\n",
    "            elif self.regularization == 'L1':\n",
    "                gradient[1:] += self.reg_coeff * np.sign(self.theta[1:])\n",
    "\n",
    "            # Update weights using gradient\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "\n",
    "            # Check for early stopping if enabled and validation data is provided\n",
    "            if self.early_stopping and X_val is not None and y_val is not None:\n",
    "                val_accuracy = self.score(X_val, y_val)\n",
    "\n",
    "                if val_accuracy > best_val_accuracy:\n",
    "                    best_val_accuracy = val_accuracy\n",
    "                    best_theta = self.theta.copy()\n",
    "                    self.best_iteration = i  # <-- Record the best iteration\n",
    "                    no_improvement_count = 0\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "\n",
    "                if no_improvement_count >= self.patience:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Early stopping after {i} iterations. Best iteration was {self.best_iteration}.\")\n",
    "                    self.theta = best_theta\n",
    "                    break\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for given features\n",
    "        \n",
    "        Parameters:\n",
    "        - X: data features\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted class labels\n",
    "        \"\"\"\n",
    "        # Convert to sparse matrix if not already\n",
    "        if not isinstance(X, csr_matrix):\n",
    "            X = csr_matrix(X)\n",
    "\n",
    "        # Add bias term (intercept) to data\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        X_bias = np.hstack([bias, X.toarray()])\n",
    "\n",
    "        # Compute class scores and return predictions\n",
    "        scores = X_bias.dot(self.theta)\n",
    "        predictions = np.argmax(scores, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes accuracy of predictions\n",
    "        \n",
    "        Parameters:\n",
    "        - X: data features\n",
    "        - y: true class labels\n",
    "        \n",
    "        Returns:\n",
    "        - Accuracy of predictions\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation bayésienne des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-04 00:15:46,294] A new study created in memory with name: no-name-dbf96dc7-9f38-442f-af5e-a21a4d32d8de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-04 00:23:29,401] Trial 0 finished with value: 0.7626453488372092 and parameters: {'weights': True, 'regularization_index': 1, 'reg_coeff': 0.004694877489240357}. Best is trial 0 with value: 0.7626453488372092.\n",
      "[I 2023-11-04 00:23:50,678] Trial 1 finished with value: 0.8046996124031008 and parameters: {'weights': False, 'regularization_index': 2, 'reg_coeff': 0.4535725735761881}. Best is trial 1 with value: 0.8046996124031008.\n",
      "[I 2023-11-04 00:32:34,953] Trial 2 finished with value: 0.7796996124031009 and parameters: {'weights': True, 'regularization_index': 2, 'reg_coeff': 0.1467906434770233}. Best is trial 1 with value: 0.8046996124031008.\n",
      "[I 2023-11-04 00:41:40,776] Trial 3 finished with value: 0.7758236434108529 and parameters: {'weights': True, 'regularization_index': 2, 'reg_coeff': 0.1704584342722788}. Best is trial 1 with value: 0.8046996124031008.\n",
      "[I 2023-11-04 00:42:00,833] Trial 4 finished with value: 0.7316860465116278 and parameters: {'weights': False, 'regularization_index': 1, 'reg_coeff': 0.29401690295007693}. Best is trial 1 with value: 0.8046996124031008.\n",
      "[I 2023-11-04 00:46:10,176] Trial 5 finished with value: 0.8296511627906976 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 5 with value: 0.8296511627906976.\n",
      "[I 2023-11-04 00:51:30,058] Trial 6 finished with value: 0.7774709302325581 and parameters: {'weights': True, 'regularization_index': 2, 'reg_coeff': 0.4992914211018189}. Best is trial 5 with value: 0.8296511627906976.\n",
      "[I 2023-11-04 00:51:50,389] Trial 7 finished with value: 0.8012596899224805 and parameters: {'weights': False, 'regularization_index': 2, 'reg_coeff': 0.3523658803369668}. Best is trial 5 with value: 0.8296511627906976.\n",
      "[I 2023-11-04 00:55:11,978] Trial 8 pruned. \n",
      "[I 2023-11-04 00:55:27,549] Trial 9 pruned. \n",
      "[I 2023-11-04 00:58:00,102] Trial 10 finished with value: 0.819718992248062 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 5 with value: 0.8296511627906976.\n",
      "[I 2023-11-04 01:00:35,590] Trial 11 finished with value: 0.8313953488372093 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 11 with value: 0.8313953488372093.\n",
      "[I 2023-11-04 01:02:00,086] Trial 12 finished with value: 0.8308139534883722 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 11 with value: 0.8313953488372093.\n",
      "[I 2023-11-04 01:04:18,999] Trial 13 finished with value: 0.8276647286821706 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 11 with value: 0.8313953488372093.\n",
      "[I 2023-11-04 01:06:15,519] Trial 14 finished with value: 0.8326550387596899 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:07:16,402] Trial 15 finished with value: 0.8253391472868218 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:07:22,403] Trial 16 pruned. \n",
      "[I 2023-11-04 01:08:09,365] Trial 17 finished with value: 0.8250484496124031 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:08:15,345] Trial 18 pruned. \n",
      "[I 2023-11-04 01:10:12,639] Trial 19 finished with value: 0.8286821705426357 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:11:53,359] Trial 20 finished with value: 0.8306686046511628 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:14:06,515] Trial 21 finished with value: 0.8260174418604652 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:15:41,099] Trial 22 finished with value: 0.8255329457364341 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:16:20,139] Trial 23 pruned. \n",
      "[I 2023-11-04 01:16:26,321] Trial 24 pruned. \n",
      "[I 2023-11-04 01:16:52,967] Trial 25 pruned. \n",
      "[I 2023-11-04 01:19:05,737] Trial 26 pruned. \n",
      "[I 2023-11-04 01:19:12,245] Trial 27 pruned. \n",
      "[I 2023-11-04 01:19:33,249] Trial 28 pruned. \n",
      "[I 2023-11-04 01:19:43,288] Trial 29 pruned. \n",
      "[I 2023-11-04 01:21:53,089] Trial 30 finished with value: 0.8308624031007752 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:23:13,377] Trial 31 finished with value: 0.8291182170542637 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:24:55,833] Trial 32 finished with value: 0.8310562015503876 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:25:27,786] Trial 33 finished with value: 0.8275678294573643 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:27:38,000] Trial 34 pruned. \n",
      "[I 2023-11-04 01:29:47,913] Trial 35 pruned. \n",
      "[I 2023-11-04 01:29:54,014] Trial 36 pruned. \n",
      "[I 2023-11-04 01:30:40,707] Trial 37 pruned. \n",
      "[I 2023-11-04 01:32:22,284] Trial 38 pruned. \n",
      "[I 2023-11-04 01:32:28,448] Trial 39 pruned. \n",
      "[I 2023-11-04 01:32:36,820] Trial 40 pruned. \n",
      "[I 2023-11-04 01:32:55,435] Trial 41 pruned. \n",
      "[I 2023-11-04 01:33:31,835] Trial 42 pruned. \n",
      "[I 2023-11-04 01:33:44,472] Trial 43 pruned. \n",
      "[I 2023-11-04 01:34:01,817] Trial 44 pruned. \n",
      "[I 2023-11-04 01:34:24,672] Trial 45 pruned. \n",
      "[I 2023-11-04 01:34:34,304] Trial 46 pruned. \n",
      "[I 2023-11-04 01:36:22,610] Trial 47 finished with value: 0.8322189922480621 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:36:35,878] Trial 48 pruned. \n",
      "[I 2023-11-04 01:36:41,961] Trial 49 pruned. \n",
      "[I 2023-11-04 01:37:01,984] Trial 50 pruned. \n",
      "[I 2023-11-04 01:37:26,386] Trial 51 pruned. \n",
      "[I 2023-11-04 01:38:01,205] Trial 52 pruned. \n",
      "[I 2023-11-04 01:38:39,304] Trial 53 pruned. \n",
      "[I 2023-11-04 01:39:43,006] Trial 54 pruned. \n",
      "[I 2023-11-04 01:40:11,637] Trial 55 pruned. \n",
      "[I 2023-11-04 01:40:34,141] Trial 56 pruned. \n",
      "[I 2023-11-04 01:41:50,039] Trial 57 pruned. \n",
      "[I 2023-11-04 01:42:12,701] Trial 58 pruned. \n",
      "[I 2023-11-04 01:42:19,046] Trial 59 pruned. \n",
      "[I 2023-11-04 01:42:27,936] Trial 60 pruned. \n",
      "[I 2023-11-04 01:43:21,979] Trial 61 pruned. \n",
      "[I 2023-11-04 01:46:27,676] Trial 62 finished with value: 0.8266472868217054 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:46:55,221] Trial 63 pruned. \n",
      "[I 2023-11-04 01:48:25,892] Trial 64 finished with value: 0.8348837209302326 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 01:48:39,277] Trial 65 pruned. \n",
      "[I 2023-11-04 01:49:23,566] Trial 66 pruned. \n",
      "[I 2023-11-04 01:50:59,928] Trial 67 finished with value: 0.8299903100775193 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 01:51:43,255] Trial 68 pruned. \n",
      "[I 2023-11-04 01:52:21,883] Trial 69 finished with value: 0.8276162790697675 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 01:52:28,086] Trial 70 pruned. \n",
      "[I 2023-11-04 01:52:49,351] Trial 71 pruned. \n",
      "[I 2023-11-04 01:53:23,044] Trial 72 pruned. \n",
      "[I 2023-11-04 01:53:37,205] Trial 73 pruned. \n",
      "[I 2023-11-04 01:53:55,609] Trial 74 pruned. \n",
      "[I 2023-11-04 01:54:16,670] Trial 75 pruned. \n",
      "[I 2023-11-04 01:56:05,195] Trial 76 finished with value: 0.8273255813953488 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 01:56:20,566] Trial 77 pruned. \n",
      "[I 2023-11-04 01:56:41,289] Trial 78 pruned. \n",
      "[I 2023-11-04 01:59:03,892] Trial 79 finished with value: 0.823498062015504 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:00:57,577] Trial 80 pruned. \n",
      "[I 2023-11-04 02:01:14,407] Trial 81 pruned. \n",
      "[I 2023-11-04 02:01:37,367] Trial 82 pruned. \n",
      "[I 2023-11-04 02:01:45,871] Trial 83 pruned. \n",
      "[I 2023-11-04 02:02:41,533] Trial 84 pruned. \n",
      "[I 2023-11-04 02:04:17,058] Trial 85 finished with value: 0.828391472868217 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:05:48,848] Trial 86 finished with value: 0.8330426356589147 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:07:31,432] Trial 87 finished with value: 0.8288759689922481 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:07:47,991] Trial 88 pruned. \n",
      "[I 2023-11-04 02:07:54,073] Trial 89 pruned. \n",
      "[I 2023-11-04 02:08:09,473] Trial 90 pruned. \n",
      "[I 2023-11-04 02:08:56,195] Trial 91 pruned. \n",
      "[I 2023-11-04 02:09:11,049] Trial 92 pruned. \n",
      "[I 2023-11-04 02:10:24,155] Trial 93 finished with value: 0.827858527131783 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:10:32,466] Trial 94 pruned. \n",
      "[I 2023-11-04 02:11:14,929] Trial 95 pruned. \n",
      "[I 2023-11-04 02:12:10,207] Trial 96 pruned. \n",
      "[I 2023-11-04 02:12:30,906] Trial 97 pruned. \n",
      "[I 2023-11-04 02:14:54,894] Trial 98 pruned. \n",
      "[I 2023-11-04 02:16:16,492] Trial 99 finished with value: 0.8307170542635659 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:16:42,992] Trial 100 pruned. \n",
      "[I 2023-11-04 02:17:38,030] Trial 101 pruned. \n",
      "[I 2023-11-04 02:18:04,059] Trial 102 pruned. \n",
      "[I 2023-11-04 02:19:12,473] Trial 103 finished with value: 0.8294089147286822 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:20:04,966] Trial 104 finished with value: 0.8321705426356588 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:22:10,018] Trial 105 finished with value: 0.8290697674418605 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:23:05,785] Trial 106 pruned. \n",
      "[I 2023-11-04 02:23:24,024] Trial 107 pruned. \n",
      "[I 2023-11-04 02:25:03,363] Trial 108 finished with value: 0.8243217054263566 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:26:15,065] Trial 109 pruned. \n",
      "[I 2023-11-04 02:27:11,960] Trial 110 pruned. \n",
      "[I 2023-11-04 02:27:42,259] Trial 111 pruned. \n",
      "[I 2023-11-04 02:27:54,954] Trial 112 pruned. \n",
      "[I 2023-11-04 02:28:25,423] Trial 113 pruned. \n",
      "[I 2023-11-04 02:28:45,315] Trial 114 pruned. \n",
      "[I 2023-11-04 02:29:49,558] Trial 115 pruned. \n",
      "[I 2023-11-04 02:31:36,143] Trial 116 pruned. \n",
      "[I 2023-11-04 02:31:45,467] Trial 117 pruned. \n",
      "[I 2023-11-04 02:32:50,327] Trial 118 pruned. \n",
      "[I 2023-11-04 02:34:24,729] Trial 119 finished with value: 0.829796511627907 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:34:47,118] Trial 120 pruned. \n",
      "[I 2023-11-04 02:35:00,664] Trial 121 pruned. \n",
      "[I 2023-11-04 02:35:34,867] Trial 122 pruned. \n",
      "[I 2023-11-04 02:36:43,869] Trial 123 pruned. \n",
      "[I 2023-11-04 02:37:03,440] Trial 124 pruned. \n",
      "[I 2023-11-04 02:37:26,189] Trial 125 pruned. \n",
      "[I 2023-11-04 02:37:44,740] Trial 126 pruned. \n",
      "[I 2023-11-04 02:38:10,082] Trial 127 pruned. \n",
      "[I 2023-11-04 02:38:25,443] Trial 128 pruned. \n",
      "[I 2023-11-04 02:39:34,408] Trial 129 finished with value: 0.8280038759689923 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:40:11,109] Trial 130 pruned. \n",
      "[I 2023-11-04 02:40:52,033] Trial 131 pruned. \n",
      "[I 2023-11-04 02:42:57,563] Trial 132 finished with value: 0.8308624031007752 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:43:07,176] Trial 133 pruned. \n",
      "[I 2023-11-04 02:44:27,015] Trial 134 finished with value: 0.829796511627907 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:44:39,270] Trial 135 pruned. \n",
      "[I 2023-11-04 02:44:45,487] Trial 136 pruned. \n",
      "[I 2023-11-04 02:44:51,480] Trial 137 pruned. \n",
      "[I 2023-11-04 02:46:06,889] Trial 138 pruned. \n",
      "[I 2023-11-04 02:46:37,003] Trial 139 pruned. \n",
      "[I 2023-11-04 02:48:10,797] Trial 140 finished with value: 0.8314922480620156 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:48:28,144] Trial 141 pruned. \n",
      "[I 2023-11-04 02:49:07,916] Trial 142 pruned. \n",
      "[I 2023-11-04 02:50:46,616] Trial 143 finished with value: 0.82640503875969 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:51:31,548] Trial 144 pruned. \n",
      "[I 2023-11-04 02:51:37,583] Trial 145 pruned. \n",
      "[I 2023-11-04 02:51:53,997] Trial 146 pruned. \n",
      "[I 2023-11-04 02:52:33,510] Trial 147 pruned. \n",
      "[I 2023-11-04 02:52:43,135] Trial 148 pruned. \n",
      "[I 2023-11-04 02:55:44,266] Trial 149 finished with value: 0.8311531007751939 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial:\n",
      "  Value (Accuracy): 0.8349\n",
      "  weights: No\n",
      "  regularization: None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Assuming SoftmaxRegression class is defined/imported from above\n",
    "\n",
    "# Prepare your data\n",
    "X = train_data.drop(['Label'], axis=1)\n",
    "y = train_data['Label']\n",
    "regularizations = ['None', 'L1', 'L2']  # Assuming this is a global variable\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Hyperparameter setting\n",
    "    use_weights = trial.suggest_categorical('weights', [True, False])\n",
    "    regularization_index = trial.suggest_int('regularization_index', 0, 2)\n",
    "    regularization = regularizations[regularization_index]\n",
    "    \n",
    "    if regularization == 'None':\n",
    "        reg_coeff = 0  # No regularization coefficient needed\n",
    "    else:\n",
    "        reg_coeff = trial.suggest_float('reg_coeff', 0.0, 0.5)\n",
    "    \n",
    "    # Storage for scores\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        # Splitting data\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "        # Data scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "        # Data whitening using PCA\n",
    "        pca = PCA(whiten=True)\n",
    "        X_train_whitened = pca.fit_transform(X_train_scaled)\n",
    "        X_valid_whitened = pca.transform(X_valid_scaled)\n",
    "\n",
    "        # Model training\n",
    "        regressor = SoftmaxRegression(n_iterations=1_000_000, reg_coeff=reg_coeff, regularization=regularization, early_stopping=True, weights=use_weights, verbose=False) \n",
    "        regressor.fit(X_train_whitened, y_train, X_valid_whitened, y_valid)\n",
    "\n",
    "        # Validation\n",
    "        preds = regressor.predict(X_valid_whitened)\n",
    "        score = accuracy_score(y_valid, preds)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Report the accuracy score and check for pruning\n",
    "        trial.report(score, i)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    avg_accuracy = np.mean(scores)\n",
    "    return avg_accuracy\n",
    "\n",
    "def save_intermediate_results(study, trial):\n",
    "    \"\"\"Callback to save the trials dataframe after each iteration.\"\"\"\n",
    "    df = study.trials_dataframe()\n",
    "    df.to_csv(\"bayes/softmax_regression_optimization_results.csv\", index=False)\n",
    "\n",
    "# Create a study object and specify the direction is 'maximize'.\n",
    "study = optuna.create_study(direction='maximize', pruner=MedianPruner())\n",
    "study.optimize(objective, n_trials=150, callbacks=[save_intermediate_results])\n",
    "\n",
    "# Save the final results\n",
    "final_results = study.trials_dataframe()\n",
    "final_results.to_csv(\"bayes/softmax_regression_optimization_results.csv\", index=False)\n",
    "\n",
    "# Print the best result\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value (Accuracy): {best_trial.value:.4f}\")\n",
    "for key, value in best_trial.params.items():\n",
    "    if key == 'regularization_index':\n",
    "        print(f\"  regularization: {regularizations[int(value)]}\")\n",
    "    elif key == 'weights':\n",
    "        print(f\"  {key}: {'Yes' if value else 'No'}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggrégation des prédictions pour 500 modèles différents avec les meilleurs hyperparamètres afin de réduire la variance des prédictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normal</th>\n",
       "      <th>Tropical Cyclone</th>\n",
       "      <th>Atmospheric River</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.924998</td>\n",
       "      <td>0.744610</td>\n",
       "      <td>0.564430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.833137</td>\n",
       "      <td>0.834148</td>\n",
       "      <td>0.785644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.924998</td>\n",
       "      <td>0.744610</td>\n",
       "      <td>0.564430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.876624</td>\n",
       "      <td>0.785904</td>\n",
       "      <td>0.656131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Normal  Tropical Cyclone  Atmospheric River\n",
       "Accuracy   0.924998          0.744610           0.564430\n",
       "Precision  0.833137          0.834148           0.785644\n",
       "Recall     0.924998          0.744610           0.564430\n",
       "F1         0.876624          0.785904           0.656131"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_data.drop('Label', axis=1)\n",
    "y = train_data['Label']\n",
    "\n",
    "total_predictions = np.zeros((10320, 500))\n",
    "\n",
    "# Initialize a dictionary to hold accumulated metrics\n",
    "accumulated_metrics = {\n",
    "    'Precision': {0: [], 1: [], 2: []},\n",
    "    'Recall': {0: [], 1: [], 2: []},\n",
    "    'F1': {0: [], 1: [], 2: []},\n",
    "    'Accuracy': {0: [], 1: [], 2: []}\n",
    "}\n",
    "\n",
    "for i in range(500):\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "    # Apply PCA whitening\n",
    "    pca = PCA(whiten=True)\n",
    "    X_train_whitened = pca.fit_transform(X_train_scaled)\n",
    "    X_valid_whitened = pca.transform(X_valid_scaled)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    regressor = SoftmaxRegression(n_iterations=1_000_000, reg_coeff=0, regularization=\"None\", early_stopping=True, weights=False, verbose=False)\n",
    "    regressor.fit(X_train_whitened, y_train, X_valid_whitened, y_valid)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred = regressor.predict(X_valid_whitened)\n",
    "\n",
    "    # Generate a classification report for this fold\n",
    "    report = classification_report(y_valid, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Calculate per-class accuracy and update accumulated metrics\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    for label in range(len(cm)):\n",
    "        label_str = str(label)\n",
    "        accumulated_metrics['Accuracy'][label].append(cm[label, label] / np.sum(cm[label]))\n",
    "        accumulated_metrics['Precision'][label].append(report[label_str]['precision'])\n",
    "        accumulated_metrics['Recall'][label].append(report[label_str]['recall'])\n",
    "        accumulated_metrics['F1'][label].append(report[label_str]['f1-score'])\n",
    "\n",
    "    # Predict on the test set\n",
    "    X_test = scaler.transform(test_data)\n",
    "    X_test_whitened = pca.transform(X_test)\n",
    "    total_predictions[:, i] = regressor.predict(X_test_whitened)\n",
    "\n",
    "# Calculate the average of the accumulated metrics\n",
    "average_metrics = {measure: {} for measure in accumulated_metrics}\n",
    "for measure, classes in accumulated_metrics.items():\n",
    "    for class_label, values in classes.items():\n",
    "        average_metrics[measure][class_label] = np.mean(values)\n",
    "\n",
    "# Remap class labels to the desired names\n",
    "class_label_names = {0: 'Normal', 1: 'Tropical Cyclone', 2: 'Atmospheric River'}\n",
    "\n",
    "# Now create a new DataFrame that will contain the remapped labels and measures\n",
    "final_metrics_df = pd.DataFrame()\n",
    "\n",
    "# Populate the new DataFrame with the remapped labels and the average metrics\n",
    "for measure, classes in average_metrics.items():\n",
    "    for class_label, value in classes.items():\n",
    "        final_metrics_df.at[class_label_names[class_label], measure] = value\n",
    "\n",
    "# Reorder the DataFrame to have accuracy as the first row and performance measures as columns\n",
    "reordered_measures = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "final_metrics_df = final_metrics_df[reordered_measures]\n",
    "\n",
    "# Transpose the DataFrame to match the requested format with performance measures as rows\n",
    "final_metrics_df = final_metrics_df.T\n",
    "\n",
    "# Print the final DataFrame\n",
    "final_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to integers if necessary\n",
    "total_predictions_int = total_predictions.astype(int)\n",
    "\n",
    "# Now apply the majority voting\n",
    "y_test = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=1, arr=total_predictions_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 7931 occurrences\n",
      "Label 1: 813 occurrences\n",
      "Label 2: 1576 occurrences\n"
     ]
    }
   ],
   "source": [
    "labels, counts = np.unique(y_test, return_counts=True)\n",
    "\n",
    "for label, count in zip(labels, counts):\n",
    "    print(f\"Label {label}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'SNo': range(1, len(y_test) + 1),\n",
    "    'Label': y_test\n",
    "})\n",
    "\n",
    "df.to_csv(\"logistic_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
