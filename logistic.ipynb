{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from format_data import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation et formattage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/GAN_train.csv\")\n",
    "train_data = format_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe pour la régression logistique en utilisant seulement numpy et scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, n_iterations=100000, regularization=None, reg_coeff=0.01, \n",
    "                 weights=False, early_stopping=True, patience=500, verbose=False):\n",
    "        \"\"\"\n",
    "        Softmax Regression Classifier Initialization\n",
    "        \n",
    "        Parameters:\n",
    "        - n_iterations: maximum number of iterations for optimization\n",
    "        - regularization: regularization type ('L1' or 'L2')\n",
    "        - reg_coeff: regularization coefficient\n",
    "        - weights: boolean indicating if instance weights should be used\n",
    "        - early_stopping: boolean indicating if early stopping should be used\n",
    "        - patience: number of iterations without improvement to trigger early stopping\n",
    "        \"\"\"\n",
    "        self.learning_rate = None  # We'll be using the custom learning rate function\n",
    "        self.n_iterations = n_iterations\n",
    "        self.regularization = regularization\n",
    "        self.reg_coeff = reg_coeff\n",
    "        self.use_weights = weights\n",
    "        self.sample_weights = None\n",
    "        self.theta = None\n",
    "        self.early_stopping = early_stopping\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.best_iteration = None\n",
    "\n",
    "    def softmax(self, scores):\n",
    "        \"\"\"\n",
    "        Computes softmax probabilities for given scores\n",
    "        \n",
    "        Parameters:\n",
    "        - scores: raw score values\n",
    "        \n",
    "        Returns:\n",
    "        - Softmax probabilities\n",
    "        \"\"\"\n",
    "        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_class_weights(self, y):\n",
    "        \"\"\"\n",
    "        Computes weights for instances based on their class\n",
    "        \n",
    "        Parameters:\n",
    "        - y: class labels\n",
    "        \n",
    "        Returns:\n",
    "        - Array of instance weights\n",
    "        \"\"\"\n",
    "        class_sample_counts = np.bincount(y)\n",
    "        weights = 1. / class_sample_counts\n",
    "        weights = weights / np.sum(weights) * len(np.unique(y))\n",
    "        return np.array([weights[label] for label in y])\n",
    "\n",
    "    def get_custom_learning_rate(self, iteration):\n",
    "        \"\"\"Get custom learning rate based on current iteration\"\"\"\n",
    "        if iteration < 25:\n",
    "            return 0.1\n",
    "        elif iteration < 50:\n",
    "            return 0.05\n",
    "        elif iteration < 150:\n",
    "            return 0.01\n",
    "        elif iteration < 300:\n",
    "            return 0.005\n",
    "        else:\n",
    "            return 0.001\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Trains the softmax regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - X_train: training data features\n",
    "        - y_train: training data labels\n",
    "        - X_val: validation data features (optional)\n",
    "        - y_val: validation data labels (optional)\n",
    "        \"\"\"\n",
    "        # Convert to sparse matrix if not already\n",
    "        if not isinstance(X_train, csr_matrix):\n",
    "            X_train = csr_matrix(X_train)\n",
    "\n",
    "        # Add bias term (intercept) to training data\n",
    "        bias_train = csr_matrix(np.ones((X_train.shape[0], 1)))\n",
    "        X_train_bias = hstack([bias_train, X_train])\n",
    "\n",
    "        n_samples, n_features = X_train_bias.shape\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        \n",
    "        # Initialize weights with small random values\n",
    "        self.theta = np.random.randn(n_features, n_classes) * 0.01\n",
    "\n",
    "        # Compute or set sample weights\n",
    "        if self.use_weights:\n",
    "            self.sample_weights = self.compute_class_weights(y_train)\n",
    "        else:\n",
    "            self.sample_weights = np.ones(n_samples)\n",
    "\n",
    "        # Convert labels to one-hot encoding\n",
    "        y_onehot = np.zeros((n_samples, n_classes))\n",
    "        y_onehot[np.arange(n_samples), y_train] = 1\n",
    "\n",
    "        best_theta = None\n",
    "        best_val_accuracy = float('-inf')\n",
    "        no_improvement_count = 0\n",
    "\n",
    "        # Prepare validation data if early stopping is enabled\n",
    "        if self.early_stopping and X_val is not None and y_val is not None:\n",
    "            if not isinstance(X_val, csr_matrix):\n",
    "                X_val = csr_matrix(X_val)\n",
    "            bias_val = csr_matrix(np.ones((X_val.shape[0], 1)))\n",
    "            X_val_bias = hstack([bias_val, X_val])\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            # Update learning rate using custom logic\n",
    "            self.learning_rate = self.get_custom_learning_rate(i)\n",
    "\n",
    "            # Compute predictions using current weights\n",
    "            scores = X_train_bias.dot(self.theta)\n",
    "            probabilities = self.softmax(scores)\n",
    "\n",
    "            # Compute gradient for optimization\n",
    "            diff_weighted = self.sample_weights[:, np.newaxis] * (y_onehot - probabilities)\n",
    "            gradient = -X_train_bias.T.dot(diff_weighted) / n_samples\n",
    "\n",
    "            # Apply regularization to gradient if specified\n",
    "            if self.regularization == 'L2':\n",
    "                gradient[1:] += self.reg_coeff * self.theta[1:]\n",
    "            elif self.regularization == 'L1':\n",
    "                gradient[1:] += self.reg_coeff * np.sign(self.theta[1:])\n",
    "\n",
    "            # Update weights using gradient\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "\n",
    "            # Check for early stopping if enabled and validation data is provided\n",
    "            if self.early_stopping and X_val is not None and y_val is not None:\n",
    "                val_accuracy = self.score(X_val, y_val)\n",
    "\n",
    "                if val_accuracy > best_val_accuracy:\n",
    "                    best_val_accuracy = val_accuracy\n",
    "                    best_theta = self.theta.copy()\n",
    "                    self.best_iteration = i  # <-- Record the best iteration\n",
    "                    no_improvement_count = 0\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "\n",
    "                if no_improvement_count >= self.patience:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Early stopping after {i} iterations. Best iteration was {self.best_iteration}.\")\n",
    "                    self.theta = best_theta\n",
    "                    break\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for given features\n",
    "        \n",
    "        Parameters:\n",
    "        - X: data features\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted class labels\n",
    "        \"\"\"\n",
    "        # Convert to sparse matrix if not already\n",
    "        if not isinstance(X, csr_matrix):\n",
    "            X = csr_matrix(X)\n",
    "\n",
    "        # Add bias term (intercept) to data\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        X_bias = np.hstack([bias, X.toarray()])\n",
    "\n",
    "        # Compute class scores and return predictions\n",
    "        scores = X_bias.dot(self.theta)\n",
    "        predictions = np.argmax(scores, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes accuracy of predictions\n",
    "        \n",
    "        Parameters:\n",
    "        - X: data features\n",
    "        - y: true class labels\n",
    "        \n",
    "        Returns:\n",
    "        - Accuracy of predictions\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation bayésienne des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-04 00:15:46,294] A new study created in memory with name: no-name-dbf96dc7-9f38-442f-af5e-a21a4d32d8de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-04 00:23:29,401] Trial 0 finished with value: 0.7626453488372092 and parameters: {'weights': True, 'regularization_index': 1, 'reg_coeff': 0.004694877489240357}. Best is trial 0 with value: 0.7626453488372092.\n",
      "[I 2023-11-04 00:23:50,678] Trial 1 finished with value: 0.8046996124031008 and parameters: {'weights': False, 'regularization_index': 2, 'reg_coeff': 0.4535725735761881}. Best is trial 1 with value: 0.8046996124031008.\n",
      "[I 2023-11-04 00:32:34,953] Trial 2 finished with value: 0.7796996124031009 and parameters: {'weights': True, 'regularization_index': 2, 'reg_coeff': 0.1467906434770233}. Best is trial 1 with value: 0.8046996124031008.\n",
      "[I 2023-11-04 00:41:40,776] Trial 3 finished with value: 0.7758236434108529 and parameters: {'weights': True, 'regularization_index': 2, 'reg_coeff': 0.1704584342722788}. Best is trial 1 with value: 0.8046996124031008.\n",
      "[I 2023-11-04 00:42:00,833] Trial 4 finished with value: 0.7316860465116278 and parameters: {'weights': False, 'regularization_index': 1, 'reg_coeff': 0.29401690295007693}. Best is trial 1 with value: 0.8046996124031008.\n",
      "[I 2023-11-04 00:46:10,176] Trial 5 finished with value: 0.8296511627906976 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 5 with value: 0.8296511627906976.\n",
      "[I 2023-11-04 00:51:30,058] Trial 6 finished with value: 0.7774709302325581 and parameters: {'weights': True, 'regularization_index': 2, 'reg_coeff': 0.4992914211018189}. Best is trial 5 with value: 0.8296511627906976.\n",
      "[I 2023-11-04 00:51:50,389] Trial 7 finished with value: 0.8012596899224805 and parameters: {'weights': False, 'regularization_index': 2, 'reg_coeff': 0.3523658803369668}. Best is trial 5 with value: 0.8296511627906976.\n",
      "[I 2023-11-04 00:55:11,978] Trial 8 pruned. \n",
      "[I 2023-11-04 00:55:27,549] Trial 9 pruned. \n",
      "[I 2023-11-04 00:58:00,102] Trial 10 finished with value: 0.819718992248062 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 5 with value: 0.8296511627906976.\n",
      "[I 2023-11-04 01:00:35,590] Trial 11 finished with value: 0.8313953488372093 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 11 with value: 0.8313953488372093.\n",
      "[I 2023-11-04 01:02:00,086] Trial 12 finished with value: 0.8308139534883722 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 11 with value: 0.8313953488372093.\n",
      "[I 2023-11-04 01:04:18,999] Trial 13 finished with value: 0.8276647286821706 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 11 with value: 0.8313953488372093.\n",
      "[I 2023-11-04 01:06:15,519] Trial 14 finished with value: 0.8326550387596899 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:07:16,402] Trial 15 finished with value: 0.8253391472868218 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:07:22,403] Trial 16 pruned. \n",
      "[I 2023-11-04 01:08:09,365] Trial 17 finished with value: 0.8250484496124031 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:08:15,345] Trial 18 pruned. \n",
      "[I 2023-11-04 01:10:12,639] Trial 19 finished with value: 0.8286821705426357 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:11:53,359] Trial 20 finished with value: 0.8306686046511628 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:14:06,515] Trial 21 finished with value: 0.8260174418604652 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:15:41,099] Trial 22 finished with value: 0.8255329457364341 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:16:20,139] Trial 23 pruned. \n",
      "[I 2023-11-04 01:16:26,321] Trial 24 pruned. \n",
      "[I 2023-11-04 01:16:52,967] Trial 25 pruned. \n",
      "[I 2023-11-04 01:19:05,737] Trial 26 pruned. \n",
      "[I 2023-11-04 01:19:12,245] Trial 27 pruned. \n",
      "[I 2023-11-04 01:19:33,249] Trial 28 pruned. \n",
      "[I 2023-11-04 01:19:43,288] Trial 29 pruned. \n",
      "[I 2023-11-04 01:21:53,089] Trial 30 finished with value: 0.8308624031007752 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:23:13,377] Trial 31 finished with value: 0.8291182170542637 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:24:55,833] Trial 32 finished with value: 0.8310562015503876 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:25:27,786] Trial 33 finished with value: 0.8275678294573643 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:27:38,000] Trial 34 pruned. \n",
      "[I 2023-11-04 01:29:47,913] Trial 35 pruned. \n",
      "[I 2023-11-04 01:29:54,014] Trial 36 pruned. \n",
      "[I 2023-11-04 01:30:40,707] Trial 37 pruned. \n",
      "[I 2023-11-04 01:32:22,284] Trial 38 pruned. \n",
      "[I 2023-11-04 01:32:28,448] Trial 39 pruned. \n",
      "[I 2023-11-04 01:32:36,820] Trial 40 pruned. \n",
      "[I 2023-11-04 01:32:55,435] Trial 41 pruned. \n",
      "[I 2023-11-04 01:33:31,835] Trial 42 pruned. \n",
      "[I 2023-11-04 01:33:44,472] Trial 43 pruned. \n",
      "[I 2023-11-04 01:34:01,817] Trial 44 pruned. \n",
      "[I 2023-11-04 01:34:24,672] Trial 45 pruned. \n",
      "[I 2023-11-04 01:34:34,304] Trial 46 pruned. \n",
      "[I 2023-11-04 01:36:22,610] Trial 47 finished with value: 0.8322189922480621 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:36:35,878] Trial 48 pruned. \n",
      "[I 2023-11-04 01:36:41,961] Trial 49 pruned. \n",
      "[I 2023-11-04 01:37:01,984] Trial 50 pruned. \n",
      "[I 2023-11-04 01:37:26,386] Trial 51 pruned. \n",
      "[I 2023-11-04 01:38:01,205] Trial 52 pruned. \n",
      "[I 2023-11-04 01:38:39,304] Trial 53 pruned. \n",
      "[I 2023-11-04 01:39:43,006] Trial 54 pruned. \n",
      "[I 2023-11-04 01:40:11,637] Trial 55 pruned. \n",
      "[I 2023-11-04 01:40:34,141] Trial 56 pruned. \n",
      "[I 2023-11-04 01:41:50,039] Trial 57 pruned. \n",
      "[I 2023-11-04 01:42:12,701] Trial 58 pruned. \n",
      "[I 2023-11-04 01:42:19,046] Trial 59 pruned. \n",
      "[I 2023-11-04 01:42:27,936] Trial 60 pruned. \n",
      "[I 2023-11-04 01:43:21,979] Trial 61 pruned. \n",
      "[I 2023-11-04 01:46:27,676] Trial 62 finished with value: 0.8266472868217054 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 14 with value: 0.8326550387596899.\n",
      "[I 2023-11-04 01:46:55,221] Trial 63 pruned. \n",
      "[I 2023-11-04 01:48:25,892] Trial 64 finished with value: 0.8348837209302326 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 01:48:39,277] Trial 65 pruned. \n",
      "[I 2023-11-04 01:49:23,566] Trial 66 pruned. \n",
      "[I 2023-11-04 01:50:59,928] Trial 67 finished with value: 0.8299903100775193 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 01:51:43,255] Trial 68 pruned. \n",
      "[I 2023-11-04 01:52:21,883] Trial 69 finished with value: 0.8276162790697675 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 01:52:28,086] Trial 70 pruned. \n",
      "[I 2023-11-04 01:52:49,351] Trial 71 pruned. \n",
      "[I 2023-11-04 01:53:23,044] Trial 72 pruned. \n",
      "[I 2023-11-04 01:53:37,205] Trial 73 pruned. \n",
      "[I 2023-11-04 01:53:55,609] Trial 74 pruned. \n",
      "[I 2023-11-04 01:54:16,670] Trial 75 pruned. \n",
      "[I 2023-11-04 01:56:05,195] Trial 76 finished with value: 0.8273255813953488 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 01:56:20,566] Trial 77 pruned. \n",
      "[I 2023-11-04 01:56:41,289] Trial 78 pruned. \n",
      "[I 2023-11-04 01:59:03,892] Trial 79 finished with value: 0.823498062015504 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:00:57,577] Trial 80 pruned. \n",
      "[I 2023-11-04 02:01:14,407] Trial 81 pruned. \n",
      "[I 2023-11-04 02:01:37,367] Trial 82 pruned. \n",
      "[I 2023-11-04 02:01:45,871] Trial 83 pruned. \n",
      "[I 2023-11-04 02:02:41,533] Trial 84 pruned. \n",
      "[I 2023-11-04 02:04:17,058] Trial 85 finished with value: 0.828391472868217 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:05:48,848] Trial 86 finished with value: 0.8330426356589147 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:07:31,432] Trial 87 finished with value: 0.8288759689922481 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:07:47,991] Trial 88 pruned. \n",
      "[I 2023-11-04 02:07:54,073] Trial 89 pruned. \n",
      "[I 2023-11-04 02:08:09,473] Trial 90 pruned. \n",
      "[I 2023-11-04 02:08:56,195] Trial 91 pruned. \n",
      "[I 2023-11-04 02:09:11,049] Trial 92 pruned. \n",
      "[I 2023-11-04 02:10:24,155] Trial 93 finished with value: 0.827858527131783 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:10:32,466] Trial 94 pruned. \n",
      "[I 2023-11-04 02:11:14,929] Trial 95 pruned. \n",
      "[I 2023-11-04 02:12:10,207] Trial 96 pruned. \n",
      "[I 2023-11-04 02:12:30,906] Trial 97 pruned. \n",
      "[I 2023-11-04 02:14:54,894] Trial 98 pruned. \n",
      "[I 2023-11-04 02:16:16,492] Trial 99 finished with value: 0.8307170542635659 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:16:42,992] Trial 100 pruned. \n",
      "[I 2023-11-04 02:17:38,030] Trial 101 pruned. \n",
      "[I 2023-11-04 02:18:04,059] Trial 102 pruned. \n",
      "[I 2023-11-04 02:19:12,473] Trial 103 finished with value: 0.8294089147286822 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:20:04,966] Trial 104 finished with value: 0.8321705426356588 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:22:10,018] Trial 105 finished with value: 0.8290697674418605 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:23:05,785] Trial 106 pruned. \n",
      "[I 2023-11-04 02:23:24,024] Trial 107 pruned. \n",
      "[I 2023-11-04 02:25:03,363] Trial 108 finished with value: 0.8243217054263566 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:26:15,065] Trial 109 pruned. \n",
      "[I 2023-11-04 02:27:11,960] Trial 110 pruned. \n",
      "[I 2023-11-04 02:27:42,259] Trial 111 pruned. \n",
      "[I 2023-11-04 02:27:54,954] Trial 112 pruned. \n",
      "[I 2023-11-04 02:28:25,423] Trial 113 pruned. \n",
      "[I 2023-11-04 02:28:45,315] Trial 114 pruned. \n",
      "[I 2023-11-04 02:29:49,558] Trial 115 pruned. \n",
      "[I 2023-11-04 02:31:36,143] Trial 116 pruned. \n",
      "[I 2023-11-04 02:31:45,467] Trial 117 pruned. \n",
      "[I 2023-11-04 02:32:50,327] Trial 118 pruned. \n",
      "[I 2023-11-04 02:34:24,729] Trial 119 finished with value: 0.829796511627907 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:34:47,118] Trial 120 pruned. \n",
      "[I 2023-11-04 02:35:00,664] Trial 121 pruned. \n",
      "[I 2023-11-04 02:35:34,867] Trial 122 pruned. \n",
      "[I 2023-11-04 02:36:43,869] Trial 123 pruned. \n",
      "[I 2023-11-04 02:37:03,440] Trial 124 pruned. \n",
      "[I 2023-11-04 02:37:26,189] Trial 125 pruned. \n",
      "[I 2023-11-04 02:37:44,740] Trial 126 pruned. \n",
      "[I 2023-11-04 02:38:10,082] Trial 127 pruned. \n",
      "[I 2023-11-04 02:38:25,443] Trial 128 pruned. \n",
      "[I 2023-11-04 02:39:34,408] Trial 129 finished with value: 0.8280038759689923 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:40:11,109] Trial 130 pruned. \n",
      "[I 2023-11-04 02:40:52,033] Trial 131 pruned. \n",
      "[I 2023-11-04 02:42:57,563] Trial 132 finished with value: 0.8308624031007752 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:43:07,176] Trial 133 pruned. \n",
      "[I 2023-11-04 02:44:27,015] Trial 134 finished with value: 0.829796511627907 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:44:39,270] Trial 135 pruned. \n",
      "[I 2023-11-04 02:44:45,487] Trial 136 pruned. \n",
      "[I 2023-11-04 02:44:51,480] Trial 137 pruned. \n",
      "[I 2023-11-04 02:46:06,889] Trial 138 pruned. \n",
      "[I 2023-11-04 02:46:37,003] Trial 139 pruned. \n",
      "[I 2023-11-04 02:48:10,797] Trial 140 finished with value: 0.8314922480620156 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:48:28,144] Trial 141 pruned. \n",
      "[I 2023-11-04 02:49:07,916] Trial 142 pruned. \n",
      "[I 2023-11-04 02:50:46,616] Trial 143 finished with value: 0.82640503875969 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n",
      "[I 2023-11-04 02:51:31,548] Trial 144 pruned. \n",
      "[I 2023-11-04 02:51:37,583] Trial 145 pruned. \n",
      "[I 2023-11-04 02:51:53,997] Trial 146 pruned. \n",
      "[I 2023-11-04 02:52:33,510] Trial 147 pruned. \n",
      "[I 2023-11-04 02:52:43,135] Trial 148 pruned. \n",
      "[I 2023-11-04 02:55:44,266] Trial 149 finished with value: 0.8311531007751939 and parameters: {'weights': False, 'regularization_index': 0}. Best is trial 64 with value: 0.8348837209302326.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial:\n",
      "  Value (Accuracy): 0.8349\n",
      "  weights: No\n",
      "  regularization: None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Assuming SoftmaxRegression class is defined/imported from above\n",
    "\n",
    "# Prepare your data\n",
    "X = train_data.drop(['Label'], axis=1)\n",
    "y = train_data['Label']\n",
    "regularizations = ['None', 'L1', 'L2']  # Assuming this is a global variable\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Hyperparameter setting\n",
    "    use_weights = trial.suggest_categorical('weights', [True, False])\n",
    "    regularization_index = trial.suggest_int('regularization_index', 0, 2)\n",
    "    regularization = regularizations[regularization_index]\n",
    "    \n",
    "    if regularization == 'None':\n",
    "        reg_coeff = 0  # No regularization coefficient needed\n",
    "    else:\n",
    "        reg_coeff = trial.suggest_float('reg_coeff', 0.0, 0.5)\n",
    "    \n",
    "    # Storage for scores\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        # Splitting data\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "        # Data scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "        # Data whitening using PCA\n",
    "        pca = PCA(whiten=True)\n",
    "        X_train_whitened = pca.fit_transform(X_train_scaled)\n",
    "        X_valid_whitened = pca.transform(X_valid_scaled)\n",
    "\n",
    "        # Model training\n",
    "        regressor = SoftmaxRegression(n_iterations=1_000_000, reg_coeff=reg_coeff, regularization=regularization, early_stopping=True, weights=use_weights, verbose=False) \n",
    "        regressor.fit(X_train_whitened, y_train, X_valid_whitened, y_valid)\n",
    "\n",
    "        # Validation\n",
    "        preds = regressor.predict(X_valid_whitened)\n",
    "        score = accuracy_score(y_valid, preds)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Report the accuracy score and check for pruning\n",
    "        trial.report(score, i)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    avg_accuracy = np.mean(scores)\n",
    "    return avg_accuracy\n",
    "\n",
    "def save_intermediate_results(study, trial):\n",
    "    \"\"\"Callback to save the trials dataframe after each iteration.\"\"\"\n",
    "    df = study.trials_dataframe()\n",
    "    df.to_csv(\"bayes/softmax_regression_optimization_results.csv\", index=False)\n",
    "\n",
    "# Create a study object and specify the direction is 'maximize'.\n",
    "study = optuna.create_study(direction='maximize', pruner=MedianPruner())\n",
    "study.optimize(objective, n_trials=150, callbacks=[save_intermediate_results])\n",
    "\n",
    "# Save the final results\n",
    "final_results = study.trials_dataframe()\n",
    "final_results.to_csv(\"bayes/softmax_regression_optimization_results.csv\", index=False)\n",
    "\n",
    "# Print the best result\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value (Accuracy): {best_trial.value:.4f}\")\n",
    "for key, value in best_trial.params.items():\n",
    "    if key == 'regularization_index':\n",
    "        print(f\"  regularization: {regularizations[int(value)]}\")\n",
    "    elif key == 'weights':\n",
    "        print(f\"  {key}: {'Yes' if value else 'No'}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggrégation des prédictions pour 100 modèles différents avec les meilleurs hyperparamètres afin de réduire la variance des prédictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 done\n",
      "Fold 2 done\n",
      "Fold 3 done\n",
      "Fold 4 done\n",
      "Fold 5 done\n",
      "Fold 6 done\n",
      "Fold 7 done\n",
      "Fold 8 done\n",
      "Fold 9 done\n",
      "Fold 10 done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.825436</td>\n",
       "      <td>0.823246</td>\n",
       "      <td>0.825436</td>\n",
       "      <td>0.817457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score\n",
       "0  0.825436   0.823246  0.825436  0.817457"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Import your custom SoftmaxRegression or an equivalent classifier if available\n",
    "# from your_library import SoftmaxRegression\n",
    "\n",
    "# Load and format your data\n",
    "train_data = pd.read_csv(\"./data/train.csv\")  # Ensure you have a train.csv or change the path accordingly\n",
    "test_data = pd.read_csv(\"./data/test.csv\")\n",
    "# Assuming format_data is a function you've defined to format your test data\n",
    "# test_data = format_data(test_data)  # Uncomment this line if you have such a function\n",
    "\n",
    "X = train_data.drop('Label', axis=1)\n",
    "y = train_data['Label']\n",
    "\n",
    "total_predictions = np.zeros((len(test_data), 10))\n",
    "\n",
    "# Initialize lists to store metrics for validation set\n",
    "class_report = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "    # Apply PCA whitening\n",
    "    pca = PCA(whiten=True)\n",
    "    X_train_whitened = pca.fit_transform(X_train_scaled)\n",
    "    X_valid_whitened = pca.transform(X_valid_scaled)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    # Replace with your actual model and parameters\n",
    "    regressor = SoftmaxRegression(n_iterations=1_000_000, reg_coeff=0, regularization=\"None\", early_stopping=True, weights=False, verbose=False) \n",
    "    regressor.fit(X_train_whitened, y_train, X_valid_whitened, y_valid)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred = regressor.predict(X_valid_whitened)\n",
    "\n",
    "    # Generate a classification report for this fold\n",
    "    report = classification_report(y_valid, y_pred, output_dict=True, zero_division=0)\n",
    "    class_report.append(report)\n",
    "\n",
    "    # Predict on the test set\n",
    "    X_test = scaler.transform(test_data)\n",
    "    X_test_whitened = pca.transform(X_test)\n",
    "\n",
    "    total_predictions[:, i] = regressor.predict(X_test_whitened)\n",
    "\n",
    "    print(f\"Fold {i+1} done\")\n",
    "\n",
    "# Now calculate the average of the classification reports\n",
    "# Initialize dictionary to store the average of classification metrics for each class\n",
    "average_report = {}\n",
    "\n",
    "# We assume class_report is not empty and all reports have the same structure\n",
    "labels = list(class_report[0].keys())\n",
    "labels.remove('accuracy')  # We don't need to average accuracy across folds, it's not class-specific\n",
    "for label in labels[:-2]:  # The last two elements are 'macro avg' and 'weighted avg'\n",
    "    average_report[label] = {\n",
    "        'precision': np.mean([cr[label]['precision'] for cr in class_report]),\n",
    "        'recall': np.mean([cr[label]['recall'] for cr in class_report]),\n",
    "        'f1-score': np.mean([cr[label]['f1-score'] for cr in class_report])\n",
    "    }\n",
    "\n",
    "# Convert the average classification metrics dictionary to a DataFrame\n",
    "class_metrics_df = pd.DataFrame(average_report).T\n",
    "\n",
    "# Print the DataFrames to see the metrics\n",
    "class_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "    # Apply PCA whitening\n",
    "    pca = PCA(whiten=True)\n",
    "    X_train_whitened = pca.fit_transform(X_train_scaled)\n",
    "    X_valid_whitened = pca.transform(X_valid_scaled)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    # Replace with your actual model and parameters\n",
    "    regressor = SoftmaxRegression(n_iterations=1_000_000, reg_coeff=0, regularization=\"None\", early_stopping=True, weights=False, verbose=False) \n",
    "    regressor.fit(X_train_whitened, y_train, X_valid_whitened, y_valid)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred = regressor.predict(X_valid_whitened)\n",
    "\n",
    "    # Generate a classification report for this fold\n",
    "    report = classification_report(y_valid, y_pred, output_dict=True, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to integers if necessary\n",
    "total_predictions_int = total_predictions.astype(int)\n",
    "\n",
    "# Now apply the majority voting\n",
    "y_test = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=1, arr=total_predictions_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 7975 occurrences\n",
      "Label 1: 754 occurrences\n",
      "Label 2: 1591 occurrences\n"
     ]
    }
   ],
   "source": [
    "labels, counts = np.unique(y_test, return_counts=True)\n",
    "\n",
    "for label, count in zip(labels, counts):\n",
    "    print(f\"Label {label}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Utilisateur\\Documents\\MILA\\IFT 6390\\Kaggle\\Kaggle1\\logistic.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Utilisateur/Documents/MILA/IFT%206390/Kaggle/Kaggle1/logistic.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Utilisateur/Documents/MILA/IFT%206390/Kaggle/Kaggle1/logistic.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSNo\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(y_test) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Utilisateur/Documents/MILA/IFT%206390/Kaggle/Kaggle1/logistic.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m'\u001b[39m: y_test\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Utilisateur/Documents/MILA/IFT%206390/Kaggle/Kaggle1/logistic.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m })\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Utilisateur/Documents/MILA/IFT%206390/Kaggle/Kaggle1/logistic.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mlogistic_prevs.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'SNo': range(1, len(y_test) + 1),\n",
    "    'Label': y_test\n",
    "})\n",
    "\n",
    "df.to_csv(\"logistic_prevs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    7957\n",
       "2    1574\n",
       "1     789\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"logistic_prevs.csv\")\n",
    "test[\"Label\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
